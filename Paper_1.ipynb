{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score, mean_squared_error\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["##### Generate the synthetic data"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Generate synthetic data\n","X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, flip_y=0, n_redundant=0, n_informative=2, random_state=1)\n","y = np.where(y == 0, -1, 1)\n","\n","# Function to introduce label noise\n","def introduce_label_noise(y, noise_rate_positive, noise_rate_negative):\n","    y_noisy = y.copy()\n","    indices_class_1 = np.where(y == 1)[0]\n","    indices_class_minus_1 = np.where(y == -1)[0]\n","\n","    n_flip_class_1 = int(noise_rate_positive * len(indices_class_1))\n","    flip_indices_class_1 = np.random.choice(indices_class_1, size=n_flip_class_1, replace=False)\n","\n","    n_flip_class_minus_1 = int(noise_rate_negative * len(indices_class_minus_1))\n","    flip_indices_class_minus_1 = np.random.choice(indices_class_minus_1, size=n_flip_class_minus_1, replace=False)\n","\n","    y_noisy[flip_indices_class_1] = -y_noisy[flip_indices_class_1]\n","    y_noisy[flip_indices_class_minus_1] = -y_noisy[flip_indices_class_minus_1]\n","\n","    return y_noisy\n","\n","# Introduce variable noise rates\n","noise_rate_positive = 0.1 # 10% noise for class 1\n","noise_rate_negative = 0.3 # 30% noise for class -1\n","y_noisy = introduce_label_noise(y, noise_rate_positive, noise_rate_negative)"]},{"cell_type":"markdown","metadata":{"id":"1DwpjCkGH6xw"},"source":["## **Method of Unbiased Estimators**\n","\n","*This method modifies the loss function to create an unbiased estimator of the true loss despite noisy labels.*"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Unbiased Loss: 0.6931471805599452\n","Epoch 100, Unbiased Loss: 0.6124540599287328\n","Epoch 200, Unbiased Loss: 0.5747898423591798\n","Epoch 300, Unbiased Loss: 0.5550735886831\n","Epoch 400, Unbiased Loss: 0.543753776439849\n","Epoch 500, Unbiased Loss: 0.5367972077466316\n","Epoch 600, Unbiased Loss: 0.5323040341710654\n","Epoch 700, Unbiased Loss: 0.5292936373654412\n","Epoch 800, Unbiased Loss: 0.5272210776479991\n","Epoch 900, Unbiased Loss: 0.5257649039989336\n","Accuracy on clean test set with unbiased estimator loss: 0.8566666666666667\n","MSE of the model: 0.5733333333333334\n"]}],"source":["# Unbiased logistic loss function\n","def unbiased_log_loss(y_true, y_pred, rho_plus, rho_minus):\n","    log_loss = np.log(1 + np.exp(-y_true * y_pred))\n","    log_loss_neg = np.log(1 + np.exp(y_true * y_pred))\n","    unbiased_loss = ((1 - rho_minus) * log_loss - rho_plus * log_loss_neg) / (1 - rho_plus - rho_minus)\n","    return np.mean(unbiased_loss)\n","\n","# Logistic loss function and its gradient\n","def logistic_loss_and_grad(X, y, w):\n","    z = np.dot(X, w)\n","    loss = np.log(1 + np.exp(-y * z))\n","    grad = -y[:, np.newaxis] * X / (1 + np.exp(y * z))[:, np.newaxis]\n","    return np.mean(loss), np.mean(grad, axis=0)\n","\n","# Custom training loop that minimizes the unbiased loss using gradient descent.\n","def train_custom_logistic_regression(X, y, rho_plus, rho_minus, learning_rate=0.01, epochs=1000):\n","    n_samples, n_features = X.shape\n","    w = np.zeros(n_features)\n","    for epoch in range(epochs):\n","        z = np.dot(X, w)\n","        unbiased_loss_value = unbiased_log_loss(y, z, rho_plus, rho_minus)\n","        _, grad = logistic_loss_and_grad(X, y, w)\n","        w -= learning_rate * grad\n","        if epoch % 100 == 0:\n","            print(f'Epoch {epoch}, Unbiased Loss: {unbiased_loss_value}')\n","    return w\n","\n","# Prepare data for training and testing\n","X_train, X_test, y_train_clean, y_test_clean = train_test_split(X, y, test_size=0.3, random_state=42)\n","X_train, X_test, y_train_noisy, y_test_noisy = train_test_split(X, y_noisy, test_size=0.3, random_state=42)\n","\n","# Train the model\n","weights = train_custom_logistic_regression(X_train, y_train_noisy, rho_plus=noise_rate_positive, rho_minus=noise_rate_negative)\n","\n","# Predict using the trained weights\n","y_pred = np.sign(np.dot(X_test, weights))\n","print(f\"Accuracy on clean test set with unbiased estimator loss: {accuracy_score(y_test_clean, y_pred)}\")\n","print(\"MSE of the model:\", mean_squared_error(y_test_clean, y_pred))\n"]},{"cell_type":"markdown","metadata":{"id":"vFD3vuHYHovO"},"source":["## **Method of Label-dependent Costs**\n","\n","*This method adjusts the cost (or weight) of different types of errors in the loss function to account for label noise.*"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on clean test set: 0.8333333333333334\n","MSE of the logistic regression model: 0.6666666666666666\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","\n","# Calculate the label-dependent costs based on noise rates using the correct formula\n","alpha_star = (0.5 - noise_rate_negative) / (1 - noise_rate_positive - noise_rate_negative)\n","costs = {1: alpha_star, -1: 1 - alpha_star}\n","\n","# Instantiate the StandardScaler\n","scaler = StandardScaler()\n","# Scale the features to normalize the data to have zero mean and unit variance\n","X_scaled = scaler.fit_transform(X)\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train_clean, y_test_clean = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n","X_train, X_test, y_train_noisy, y_test_noisy = train_test_split(X_scaled, y_noisy, test_size=0.3, random_state=42)\n","\n","# Instantiate and train the Logistic Regression model with label-dependent costs\n","logistic_model = LogisticRegression(class_weight=costs)\n","logistic_model.fit(X_train, y_train_noisy)\n","\n","# Evaluate the model on the clean test set\n","y_pred = logistic_model.predict(X_test)\n","print(f\"Accuracy on clean test set: {accuracy_score(y_test_clean, y_pred)}\")\n","\n","print(\"MSE of the logistic regression model:\", mean_squared_error(y_test_clean, y_pred))\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPc7TUDMMwUaKoj86qM5OWo","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
